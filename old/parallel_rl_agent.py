"""
Agent RL parall√©lis√© utilisant SubprocVecEnv
H√©rite de RLAgent et override uniquement la cr√©ation de l'environnement vectoris√©
"""

from pathlib import Path
from typing import Optional
import multiprocessing as mp

from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
from stable_baselines3 import PPO

from src.models.rl_agent import RLAgent


class ParallelRLAgent(RLAgent):
    """
    Version parall√©lis√©e de RLAgent
    Acc√©l√©ration : 3-8x avec 8-16 CPU
    """

    def __init__(
        self,
        env,
        weights_dir: Optional[Path] = None,
        n_envs: Optional[int] = None,
        use_subprocess: bool = True,
        **kwargs  # Tous les autres params de RLAgent
    ):
        """
        Args:
            env: StandardCellEnv de base
            weights_dir: R√©pertoire de sauvegarde
            n_envs: Nombre d'envs parall√®les (auto si None)
            use_subprocess: True=SubprocVecEnv, False=DummyVecEnv (debug)
            **kwargs: learning_rate, batch_size, etc. (pass√©s √† RLAgent)
        """
        
        # ‚úÖ Stocker les infos de l'env AVANT vectorisation
        self._base_env = env
        self.n_envs = n_envs or self._get_optimal_n_envs()
        self.use_subprocess = use_subprocess
        
        print(f"\nüîß Configuration parall√®le:")
        print(f"   CPUs disponibles: {mp.cpu_count()}")
        print(f"   Environnements parall√®les: {self.n_envs}")
        print(f"   Mode: {'SubprocVecEnv' if use_subprocess else 'DummyVecEnv'}")
        
        # ‚úÖ Appeler le parent (qui va utiliser self.vec_env qu'on override)
        super().__init__(
            env=env,
            weights_dir=weights_dir,
            **kwargs
        )
        
        # ‚úÖ APR√àS l'init parent, remplacer vec_env par la version parall√®le
        self.vec_env = self._create_parallel_env()
        
        # ‚úÖ Recr√©er le mod√®le avec le nouveau vec_env
        self.model = self._create_new_model_parallel()

    def _get_optimal_n_envs(self) -> int:
        """Auto-d√©tecte le nombre optimal d'environnements"""
        cpu_count = mp.cpu_count()
        # Laisser 2 CPU pour le syst√®me
        return max(1, min(cpu_count - 2, 12))

    def _create_parallel_env(self):
        """Cr√©e l'environnement vectoris√© parall√®le"""
        
        if self.n_envs == 1:
            # Mode single-env (pas de parall√©lisation)
            return DummyVecEnv([lambda: self._base_env])
        
        print(f"üöÄ Cr√©ation de {self.n_envs} environnements parall√®les...")
        
        # ‚úÖ Cr√©er les fonctions factory
        env_fns = [self._make_env_fn(i) for i in range(self.n_envs)]
        
        if self.use_subprocess:
            vec_env = SubprocVecEnv(env_fns, start_method='fork')
            print(f"   ‚úÖ SubprocVecEnv cr√©√© ({self.n_envs} processus)")
        else:
            vec_env = DummyVecEnv(env_fns)
            print(f"   ‚úÖ DummyVecEnv cr√©√© ({self.n_envs} envs s√©quentiels)")
        
        return vec_env

    def _make_env_fn(self, rank: int):
        """
        Factory pour cr√©er un environnement avec seed unique
        
        Args:
            rank: Index de l'environnement (0 √† n_envs-1)
        """
        from src.environment.gym_env import StandardCellEnv
        
        # ‚úÖ Capturer tous les param√®tres
        cell_name = self._base_env.cell_name
        pdk = self._base_env.pdk
        config = self._base_env.config
        cost_weights = self._base_env.cost_weights
        max_steps = self._base_env.max_steps
        use_cache = getattr(self._base_env, 'use_cache', True)
        
        # ‚úÖ R√©cup√©rer le seed de base (ou 42 par d√©faut)
        base_seed = getattr(self._base_env, '_seed', None)
        if base_seed is None:
            base_seed = 42
        
        def _init():
            """Cr√©e une copie ind√©pendante de l'environnement"""
            env = StandardCellEnv(
                cell_name=cell_name,
                pdk=pdk,
                config=config,
                cost_weights=cost_weights,
                max_steps=max_steps,
                verbose=False,  # ‚úÖ Pas de print dans les workers
                use_cache=use_cache,
                seed=base_seed + rank  # ‚úÖ Seed unique par worker
            )
            return env
        
        return _init


    def _create_new_model_parallel(self):
        """
        Cr√©e un mod√®le PPO adapt√© au nombre d'environnements parall√®les
        """
        
        # ‚úÖ Adapter n_steps au nombre d'envs
        # PPO collecte n_steps * n_envs exp√©riences par update
        total_steps_per_update = 2048
        n_steps = max(64, total_steps_per_update // self.n_envs)
        
        # ‚úÖ Adapter batch_size
        batch_size = min(self.batch_size, total_steps_per_update // 4)
        
        # ‚úÖ Adapter n_epochs (moins d'envs = plus d'epochs)
        n_epochs = max(self.n_epochs, 20 // max(1, self.n_envs // 4))
        
        print(f"\nü§ñ Hyperparam√®tres PPO adapt√©s:")
        print(f"   n_steps: {n_steps} (par env)")
        print(f"   batch_size: {batch_size}")
        print(f"   n_epochs: {n_epochs}")
        print(f"   Total steps/update: {n_steps * self.n_envs}")
        
        model = PPO(
            "MlpPolicy",
            self.vec_env,  # ‚úÖ Utiliser le vec_env parall√®le
            learning_rate=self.learning_rate,
            n_steps=n_steps,
            batch_size=batch_size,
            n_epochs=n_epochs,
            gamma=self.gamma,
            gae_lambda=self.gae_lambda,
            clip_range=self.clip_range,
            ent_coef=self.ent_coef,
            vf_coef=self.vf_coef,
            max_grad_norm=self.max_grad_norm,
            verbose=1 if self.verbose else 0
        )
        
        return model

    def train(
        self,
        total_timesteps: int = 10_000,
        save_freq: int = 1_000,
        log_interval: int = 1,  # ‚úÖ Ajout du param√®tre
        **kwargs
    ) -> float:
        """
        Entra√Æne l'agent (version parall√©lis√©e)
        
        Args:
            total_timesteps: Nombre total de steps
            save_freq: Fr√©quence de sauvegarde (en steps)
            log_interval: Fr√©quence d'affichage (en updates PPO)
            **kwargs: Param√®tres additionnels pour model.learn()
        
        Returns:
            Meilleur co√ªt obtenu
        """
        # ‚úÖ Ajuster save_freq pour le parall√©lisme
        adjusted_save_freq = max(1, save_freq // self.n_envs)
        
        print(f"\nüöÄ Entra√Ænement parall√®le:")
        print(f"   Total timesteps: {total_timesteps:,}")
        print(f"   Steps par env: ~{total_timesteps // self.n_envs:,}")
        print(f"   Save freq (ajust√©): {adjusted_save_freq:,}")
        print(f"   Log interval: {log_interval}")
        print(f"   Speedup th√©orique: ~{self.n_envs}x")
        
        # ‚úÖ Cr√©er le callback avec save_freq ajust√©
        from src.models.rl_agent import TrainingCallback
        
        callback = TrainingCallback(
            weight_manager=self.weight_manager,
            cell_name=self.cell_name,
            save_freq=adjusted_save_freq,
            verbose=1
        )
        
        # ‚úÖ Lancer l'entra√Ænement
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            log_interval=log_interval,  
            **kwargs
        )
        
        return callback.best_cost

    def cleanup(self):
        """Ferme proprement les processus parall√®les"""
        if hasattr(self.vec_env, 'close'):
            self.vec_env.close()
            print("üßπ Environnements parall√®les ferm√©s")
