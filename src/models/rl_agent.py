# src/models/rl_agent.py
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.callbacks import BaseCallback
from pathlib import Path
import numpy as np
from .weight_manager import WeightManager
from typing import Optional, Dict, List, Tuple


class TrainingCallback(BaseCallback):
    """
    Callback pour sauvegarder les meilleurs poids pendant l'entraÃ®nement
    """

    def __init__(
        self,
        weight_manager: WeightManager,
        cell_name: str,
        save_freq: int = 1000,
        verbose: int = 0
    ):
        super().__init__(verbose)
        self.weight_manager = weight_manager
        self.cell_name = cell_name
        self.save_freq = save_freq

        self.best_cost = float('inf')
        self.best_widths = None
        self.best_metrics = None
        self.n_calls = 0

    def _on_step(self) -> bool:
        """AppelÃ© Ã  chaque step de l'environnement"""
        self.n_calls += 1

        # âœ… GÃ©rer les envs vectorisÃ©s (infos = liste)
        infos = self.locals.get('infos', [])
        if not infos:
            return True

        # âœ… Prendre la premiÃ¨re info si vectorisÃ©
        if isinstance(infos, list):
            info = infos[0]
        else:
            info = infos

        if 'cost' in info and 'widths' in info:
            cost = info['cost']
            widths = info['widths']
            metrics = info.get('metrics', {})

            # Nouveau meilleur ?
            if cost < self.best_cost:
                self.best_cost = cost
                self.best_widths = widths
                self.best_metrics = metrics

                if self.verbose:
                    print(f"  ðŸŽ¯ Step {self.n_calls}: Nouveau meilleur cost={cost:.4f}")

                # Sauvegarder immÃ©diatement
                self._save_best_weights()

            # Sauvegarde pÃ©riodique (backup)
            elif self.n_calls % self.save_freq == 0:
                if self.verbose >= 2:
                    print(f"  ðŸ’¾ Step {self.n_calls}: Sauvegarde pÃ©riodique (cost={cost:.4f})")
                self._save_current_weights(widths, metrics, cost)

        return True

    def _save_best_weights(self):
        """Sauvegarde les meilleurs poids trouvÃ©s"""
        if self.best_widths is None:
            return

        # Convertir Dict[str, float] -> List[float]
        widths_list = [self.best_widths[name] for name in sorted(self.best_widths.keys())]

        # MÃ©triques complÃ¨tes
        metrics_to_save = {
            'delay_avg': self.best_metrics.get('delay_avg', 0),
            'tplh': self.best_metrics.get('tplh', self.best_metrics.get('delay_avg', 0) * 1.2),
            'tphl': self.best_metrics.get('tphl', self.best_metrics.get('delay_avg', 0) * 0.8),
            'power_avg': self.best_metrics.get('power_avg', 0),
            'energy_dyn': self.best_metrics.get('energy_dyn', 0),
            'area': self.best_metrics.get('area', 1.0),
            'reference': {}
        }

        training_info = {
            'total_steps': self.n_calls,
            'best_cost': float(self.best_cost),
            'convergence': 'ongoing'
        }

        self.weight_manager.save_weights(
            cell_name=self.cell_name,
            widths=widths_list,
            metrics=metrics_to_save,
            training_info=training_info,
            algorithm="PPO"
        )

    def _save_current_weights(self, widths: dict, metrics: dict, cost: float):
        """Sauvegarde pÃ©riodique (backup)"""
        widths_list = [widths[name] for name in sorted(widths.keys())]

        metrics_to_save = {
            'delay_avg': metrics.get('delay_avg', 0),
            'power_avg': metrics.get('power_avg', 0),
            'energy_dyn': metrics.get('energy_dyn', 0),
            'area': metrics.get('area', 1.0),
        }

        training_info = {
            'total_steps': self.n_calls,
            'current_cost': float(cost),
            'convergence': 'backup'
        }

        # Sauvegarder dans un fichier temporaire
        backup_name = f"{self.cell_name}_backup"
        self.weight_manager.save_weights(
            cell_name=backup_name,
            widths=widths_list,
            metrics=metrics_to_save,
            training_info=training_info,
            algorithm="PPO"
        )


class RLAgent:
    """Agent PPO pour optimisation de standard cells avec sauvegarde"""

    def __init__(
        self,
        env,
        weights_dir: Path = None,
        learning_rate: float = 3e-4,
        algorithm: str = "PPO",
        load_pretrained: bool = False
    ):
        # âœ… GÃ©rer env simple ou vectorisÃ©
        if hasattr(env, 'vec_env'):
            # Environnement vectorisÃ© (VectorizedStandardCellEnv)
            self.vec_env = env.vec_env
            self.is_vectorized = True
            self.n_envs = env.n_envs
            self.cell_name = env.cell_name
            print(f"âœ… Utilisation d'environnements vectorisÃ©s ({self.n_envs} envs)")
        else:
            # Environnement simple â†’ wrapper
            self.vec_env = DummyVecEnv([lambda: env])
            self.is_vectorized = False
            self.n_envs = 1
            self.cell_name = env.cell_name

        # âœ… Normalisation (optionnel mais recommandÃ©)
        self.vec_env = VecNormalize(
            self.vec_env,
            norm_obs=True,
            norm_reward=True,
            clip_obs=10.0,
            clip_reward=10.0
        )

        self.env = env  # Garder rÃ©fÃ©rence
        self.algorithm = algorithm
        self.learning_rate = learning_rate

        # âœ… Initialiser le WeightManager
        if weights_dir:
            self.weight_manager = WeightManager(base_dir=weights_dir)
            weights_dir.mkdir(parents=True, exist_ok=True)
        else:
            self.weight_manager = WeightManager()  # Utilise le rÃ©pertoire par dÃ©faut

        self.weights_dir = weights_dir

        # Charger des poids prÃ©-entraÃ®nÃ©s ?
        initial_widths = None
        if load_pretrained:
            initial_widths = self._load_pretrained_weights()

        # âœ… CrÃ©er le modÃ¨le PPO
        self.model = PPO(
            "MlpPolicy",
            self.vec_env,
            learning_rate=learning_rate,
            n_steps=2048 // self.n_envs,  # âœ… Adapter au nombre d'envs
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            verbose=1
        )

        # Si poids chargÃ©s, initialiser l'environnement
        if initial_widths is not None:
            self._warm_start(initial_widths)

    def _load_pretrained_weights(self) -> Optional[Dict[str, float]]:
        """Charge des poids prÃ©-entraÃ®nÃ©s pour warm start"""
        print(f"ðŸ” Recherche de poids prÃ©-entraÃ®nÃ©s pour {self.cell_name}...")
        data = self.weight_manager.load_weights(self.cell_name)

        if data is None:
            # Essayer de charger depuis une cellule similaire
            category = self.weight_manager._get_category(self.cell_name)
            similar_cells = self.weight_manager.list_available_cells(category=category)

            if similar_cells:
                print(f"   Cellules similaires trouvÃ©es: {similar_cells}")
                # Charger la premiÃ¨re cellule similaire
                data = self.weight_manager.load_weights(similar_cells[0])
            else:
                print("   Aucun poids prÃ©-entraÃ®nÃ© trouvÃ©")
                return None

        if data:
            widths_list = data['optimized_widths']
            
            # âœ… RÃ©cupÃ©rer les noms de transistors depuis l'env rÃ©el
            if hasattr(self.env, 'original_widths'):
                transistor_names = sorted(self.env.original_widths.keys())
            else:
                # Pour env vectorisÃ©, accÃ©der au premier env
                transistor_names = sorted(self.vec_env.envs[0].original_widths.keys())

            # Reconstruire le dictionnaire
            if len(widths_list) == len(transistor_names):
                widths_dict = {name: width for name, width in zip(transistor_names, widths_list)}
                print(f"âœ… Poids chargÃ©s: {widths_dict}")
                return widths_dict
            else:
                print(f"âš ï¸  IncompatibilitÃ©: {len(widths_list)} poids vs {len(transistor_names)} transistors")
                return None

        return None

    def _warm_start(self, initial_widths: Dict[str, float]):
        """Initialise l'environnement avec des largeurs prÃ©-entraÃ®nÃ©es"""
        print("ðŸ”¥ Warm start avec poids prÃ©-entraÃ®nÃ©s")

        # âœ… AccÃ©der au bon environnement
        if self.is_vectorized:
            target_env = self.vec_env.envs[0]
        else:
            target_env = self.env

        # RÃ©initialiser l'environnement
        obs, _ = target_env.reset()

        # Calculer l'action pour atteindre les largeurs cibles
        for name, target_width in initial_widths.items():
            if name in target_env.current_widths:
                target_env.current_widths[name] = target_width

        print(f"   Ã‰tat initial modifiÃ©: {target_env.current_widths}")

    def train(
        self,
        total_timesteps: int = 10000,
        save_freq: int = 500,
        verbose: int = 1
    ) -> float:
        """
        EntraÃ®ne l'agent avec sauvegarde pÃ©riodique des poids

        Args:
            total_timesteps: Nombre total de steps d'entraÃ®nement
            save_freq: FrÃ©quence de sauvegarde (en steps)
            verbose: 0=silent, 1=info, 2=debug

        Returns:
            Meilleur coÃ»t obtenu
        """
        if verbose > 0:
            print(f"\nðŸš€ EntraÃ®nement {self.algorithm} sur {self.cell_name}")
            print(f"   Total timesteps: {total_timesteps}")
            print(f"   Envs parallÃ¨les: {self.n_envs}")
            print(f"   Simulations effectives: {total_timesteps * self.n_envs}")
            print(f"   Sauvegarde: tous les {save_freq} steps\n")

        # Callback pour sauvegarder les meilleurs poids
        callback = TrainingCallback(
            weight_manager=self.weight_manager,
            cell_name=self.cell_name,
            save_freq=save_freq,
            verbose=verbose
        )

        # âœ… EntraÃ®ner
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            progress_bar=False  # âœ… DÃ©sactiver la barre de progression
        )

        # Sauvegarder le modÃ¨le final
        if self.weights_dir:
            model_path = self.weights_dir / f"{self.cell_name}_final.zip"
            self.model.save(str(model_path))
            print(f"\nðŸ’¾ ModÃ¨le final sauvegardÃ©: {model_path}")

        if verbose > 0:
            print(f"\nâœ… EntraÃ®nement terminÃ©")
            print(f"   Meilleur coÃ»t: {callback.best_cost:.4f}")
            self._print_training_summary()

        return callback.best_cost

    def _print_training_summary(self):
        """Affiche un rÃ©sumÃ© des poids sauvegardÃ©s"""
        print("\nðŸ“Š RÃ‰SUMÃ‰ DES POIDS SAUVEGARDÃ‰S")
        print("="*60)

        data = self.weight_manager.load_weights(self.cell_name)
        if data:
            print(f"Cellule: {data['cell_info']['full_name']}")
            print(f"CatÃ©gorie: {data['cell_info']['category']}")
            print(f"Transistors: {data['cell_info']['n_transistors']}")
            print(f"\nMÃ©triques:")
            print(f"  DÃ©lai   : {data['metrics']['delay_avg_ps']:.2f} ps")
            print(f"  Puissance: {data['metrics']['power_avg_uw']:.3f} ÂµW")
            print(f"  Ã‰nergie  : {data['metrics']['energy_dyn_fJ']:.3f} fJ")
            print(f"  Aire (rel): {data['metrics']['area_relative']:.3f}")

            print(f"\nLargeurs optimales:")
            
            # âœ… RÃ©cupÃ©rer les noms depuis le bon env
            if hasattr(self.env, 'original_widths'):
                transistor_names = sorted(self.env.original_widths.keys())
                original_widths = self.env.original_widths
            else:
                transistor_names = sorted(self.vec_env.envs[0].original_widths.keys())
                original_widths = self.vec_env.envs[0].original_widths
            
            for i, name in enumerate(transistor_names):
                if i < len(data['optimized_widths']):
                    width = data['optimized_widths'][i]
                    original = original_widths[name]
                    delta = (width - original) / original * 100
                    print(f"  {name}: {original:.0f} nm â†’ {width:.0f} nm ({delta:+.1f}%)")

        print("="*60)

    def evaluate(
        self, 
        n_episodes: int = 10
    ) -> Tuple[float, float, List[Dict[str, float]]]:
        """
        Ã‰value l'agent et retourne les statistiques
        
        Returns:
            (mean_cost, std_cost, widths_history)
        """
        costs = []
        widths_history = []

        print(f"\nðŸ“Š Ã‰valuation sur {n_episodes} Ã©pisodes...")

        for ep in range(n_episodes):
            obs = self.vec_env.reset()
            done = False
            episode_widths = None

            while not done:
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, done, info = self.vec_env.step(action)
                
                # âœ… GÃ©rer info vectorisÃ©
                if isinstance(info, list):
                    info = info[0]
                    done = done[0]
                
                if 'widths' in info:
                    episode_widths = info['widths'].copy()

            if 'cost' in info:
                costs.append(info['cost'])
                if episode_widths:
                    widths_history.append(episode_widths)
                
                print(f"  Ã‰pisode {ep+1}: cost={info['cost']:.4f}")

        mean_cost = np.mean(costs)
        std_cost = np.std(costs)

        print(f"\nðŸ“Š Statistiques sur {n_episodes} Ã©pisodes:")
        print(f"   CoÃ»t moyen: {mean_cost:.4f} Â± {std_cost:.4f}")
        print(f"   Meilleur  : {min(costs):.4f}")
        print(f"   Pire      : {max(costs):.4f}")

        return mean_cost, std_cost, widths_history

    def save(self, path: Optional[Path] = None):
        """Sauvegarde le modÃ¨le"""
        if path is None and self.weights_dir:
            path = self.weights_dir / f"{self.cell_name}_final.zip"
        
        if path:
            self.model.save(str(path))
            print(f"ðŸ’¾ ModÃ¨le sauvegardÃ©: {path}")

    def load(self, path: Path):
        """Charge un modÃ¨le sauvegardÃ©"""
        self.model = PPO.load(str(path), env=self.vec_env)
        print(f"ðŸ“¥ ModÃ¨le chargÃ©: {path}")
